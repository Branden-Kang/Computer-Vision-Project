{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVRNyzLm/NkT/KYZ1P0hWz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@johnidouglasmarangon/ocr-tools-my-latest-study-solving-real-world-problems-with-low-quality-images-dce26cbcdf9a)"
      ],
      "metadata": {
        "id": "ovyPojjyRTOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI — OpenAI"
      ],
      "metadata": {
        "id": "HhC57xFSRaq8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4QbaAkLKRH9Y"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI # pip install openai\n",
        "from base64 import b64encode\n",
        "\n",
        "\n",
        "# Generate APIKey https://platform.openai.com/api-keys\n",
        "OPENAI_API_KEY = \"\"\n",
        "\n",
        "image_path = \"image.jpg\"\n",
        "\n",
        "with open(image_path, \"rb\") as image_file:\n",
        "   base64_image = b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"OCR this image. Do not include any markdown or code formatting.\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI — Gemini"
      ],
      "metadata": {
        "id": "WKiTPZmfRhMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image # pip install Pillow\n",
        "import google.generativeai as genai # pip install google.generativeai\n",
        "\n",
        "# Generate APIkey - https://aistudio.google.com/apikey\n",
        "GEMINI_API_KEY = \"\"\n",
        "\n",
        "image_path = \"image.jpg\" # Put here your image path\n",
        "model_name=\"gemini-1.5-flash\"\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "image_file = Image.open(image_path)\n",
        "model = genai.GenerativeModel(model_name=model_name)\n",
        "\n",
        "prompt = \"OCR this image. Do not include any markdown or code formatting.\"\n",
        "\n",
        "response = model.generate_content([prompt, image_file])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "bNqdVgRvRdnb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision AI— Google Cloud"
      ],
      "metadata": {
        "id": "bNQBrz_-Rluk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import vision # pip install google-cloud-vision\n",
        "\n",
        "\n",
        "# Put here your credentials json file\n",
        "# https://developers.google.com/workspace/guides/create-credentials?hl=en\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\"\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "with open(\"image.jpg\", \"rb\") as image_file:\n",
        "    content = image_file.read()\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "response = client.text_detection(image=image)\n",
        "\n",
        "for annotation in response.text_annotations:\n",
        "    print(\"Detected Text:\", annotation.description)"
      ],
      "metadata": {
        "id": "e8mBFdzmRj15"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision AI — Azure"
      ],
      "metadata": {
        "id": "zSjTNsftRp-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install azure-ai-vision-imageanalysis\n",
        "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
        "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "\n",
        "# Generate APIkey - https://azure.microsoft.com/en-us/products/ai-services/ai-vision\n",
        "AZURE_VISION_API_KEY = \"\"\n",
        "\n",
        "image_path = \"image.jpg\"\n",
        "\n",
        "with open(image_path, \"rb\") as f:\n",
        "    image_data = f.read()\n",
        "\n",
        "# You need to create an Azure Computer Vision AI services\n",
        "# https://portal.azure.com/\n",
        "region = \"eastus\"\n",
        "endpoint = \"https://<instance name>.cognitiveservices.azure.com/\"\n",
        "\n",
        "client = ImageAnalysisClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(AZURE_VISION_API_KEY),\n",
        "    region=region,\n",
        ")\n",
        "\n",
        "response = client.analyze(\n",
        "    image_data,\n",
        "    visual_features=[VisualFeatures.READ],\n",
        ")\n",
        "\n",
        "if response.read is not None:\n",
        "    for line in response.read.blocks[0].lines:\n",
        "        print(\"Detected Text:\", line.text)"
      ],
      "metadata": {
        "id": "Hm3IZ0VjRoZ6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Source Libraries — Tesseract"
      ],
      "metadata": {
        "id": "IWaYqTkoRuVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract # pip install pytesseract\n",
        "import cv2 # pip install opencv-contrib-python\n",
        "\n",
        "\n",
        "image = cv2.imread('image.jpg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply dinarization converting the image to black-and-white.\n",
        "_, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Tesseract allows you to specify the language of the text and configure settings\n",
        "# like page segmentation mode (PSM). For low-quality images,\n",
        "# using --psm 6 (assume a single uniform block of text)\n",
        "# or --psm 11 (sparse text) can yield better results.\n",
        "config = \"-l por --oem 1 --psm 11\"\n",
        "text = pytesseract.image_to_string(image, config=config)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "Ik_JcVDGRs05"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Source Libraries — EasyOCR"
      ],
      "metadata": {
        "id": "Jqd46KipRyR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import easyocr # !pip install easyocr\n",
        "\n",
        "\n",
        "reader = easyocr.Reader(['pt'])\n",
        "results = reader.readtext('image.jpg')\n",
        "\n",
        "for (bbox, text, confidence) in results:\n",
        "    print(f\"Detected text: {text} (Confidence: {confidence:.2f})\")"
      ],
      "metadata": {
        "id": "Dpg9DFyzRwvL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Source Libraries — Surya"
      ],
      "metadata": {
        "id": "xRJ15M0vR2TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image # pip install Pillow\n",
        "\n",
        "# pip install surya-ocr\n",
        "from surya.recognition import RecognitionPredictor\n",
        "from surya.detection import DetectionPredictor\n",
        "\n",
        "\n",
        "image_path = \"image.jpg\"\n",
        "\n",
        "image = Image.open(image_path)\n",
        "\n",
        "langs = [\"pt\"]\n",
        "recognition_predictor = RecognitionPredictor()\n",
        "detection_predictor = DetectionPredictor()\n",
        "\n",
        "predictions = recognition_predictor([image], [langs], detection_predictor)\n",
        "for prediction in predictions:\n",
        "  for line in prediction.text_lines:\n",
        "      print(line.text)"
      ],
      "metadata": {
        "id": "dCubB43HR0fO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Source Libraries — DocTR"
      ],
      "metadata": {
        "id": "m9wPmT61R7to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from doctr.io import DocumentFile # !pip install \"python-doctr[torch]\"\n",
        "from doctr.models import ocr_predictor\n",
        "\n",
        "\n",
        "images_path = \"image.jpg\"\n",
        "doc = DocumentFile.from_images(images_path)\n",
        "\n",
        "model = ocr_predictor(det_arch=\"db_resnet50\", reco_arch=\"crnn_vgg16_bn\", pretrained=True)\n",
        "\n",
        "result = model(doc)\n",
        "\n",
        "for page in result.pages:\n",
        "  for block in page.blocks:\n",
        "      for line in block.lines:\n",
        "          texts = [word.value for word in line.words]\n",
        "          print(texts)"
      ],
      "metadata": {
        "id": "bqGBND87R5Yp"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}