{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computer Vision Project about Dog Breed Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMQZdZcrOYBn5+d9Uxd7WyC"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi_JVvjb3OGs"
      },
      "source": [
        "[Reference1](https://towardsdatascience.com/build-your-first-computer-vision-project-dog-breed-classification-a622d8fc691e) <br>\n",
        "[Reference2](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148) <br>\n",
        "[Reference3](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) <br>\n",
        "[Reference4](https://github.com/tuanchris/dog-project/blob/master/dog_app.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTMVwas6w2dn"
      },
      "source": [
        "You can download the dataset [here](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaUAiOVayUuu"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_fYYPlSvvTT",
        "outputId": "9050fb87-33f9-4315-b4ad-74ccc91ba9f4"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/Medium')\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content\n",
            "/content/gdrive/My Drive/Medium\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMvIwh3Xxzvu"
      },
      "source": [
        "from sklearn.datasets import load_files       \n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# define function to load train, test, and validation datasets\n",
        "def load_dataset(path):\n",
        "    data = load_files(path)\n",
        "    dog_files = np.array(data['filenames'])\n",
        "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
        "    return dog_files, dog_targets\n",
        "    \n",
        "# load train, test, and validation datasets\n",
        "train_files, train_targets = load_dataset('dogImages/train')\n",
        "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
        "test_files, test_targets = load_dataset('dogImages/test')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK5dVzJc55BT",
        "outputId": "1c625534-577f-425e-dbdd-7bbe8c015a91"
      },
      "source": [
        "train_files"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['dogImages/train/095.Kuvasz/Kuvasz_06442.jpg',\n",
              "       'dogImages/train/057.Dalmatian/Dalmatian_04054.jpg',\n",
              "       'dogImages/train/088.Irish_water_spaniel/Irish_water_spaniel_06014.jpg',\n",
              "       ..., 'dogImages/train/029.Border_collie/Border_collie_02069.jpg',\n",
              "       'dogImages/train/046.Cavalier_king_charles_spaniel/Cavalier_king_charles_spaniel_03261.jpg',\n",
              "       'dogImages/train/048.Chihuahua/Chihuahua_03416.jpg'], dtype='<U99')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SATG1OZi57Ld",
        "outputId": "4773c674-315e-4e24-f4a5-2b70d5dae192"
      },
      "source": [
        "train_targets"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6YCE7sJ4Zq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e322a1c7-5363-4138-a243-37532ed04d08"
      },
      "source": [
        "# load list of dog names\n",
        "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
        "\n",
        "# print statistics about the dataset\n",
        "print('There are %d total dog categories.' % len(dog_names))\n",
        "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
        "print('There are %d training dog images.' % len(train_files))\n",
        "print('There are %d validation dog images.' % len(valid_files))\n",
        "print('There are %d test dog images.'% len(test_files))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 133 total dog categories.\n",
            "There are 8351 total dog images.\n",
            "\n",
            "There are 6680 training dog images.\n",
            "There are 835 validation dog images.\n",
            "There are 836 test dog images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf6kspaVx8Ue"
      },
      "source": [
        "from keras.preprocessing import image                  \n",
        "from tqdm import tqdm\n",
        "\n",
        "def path_to_tensor(img_path):\n",
        "    # loads RGB image as PIL.Image.Image type\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
        "    x = image.img_to_array(img)\n",
        "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
        "    return np.expand_dims(x, axis=0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "372yrOeByFej"
      },
      "source": [
        "def paths_to_tensor(img_paths):\n",
        "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "    return np.vstack(list_of_tensors)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI1FgOOXyJq-"
      },
      "source": [
        "# Pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmyXFqXZyJ_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4126d3e-5529-4a4a-c9f6-e6acc64a8863"
      },
      "source": [
        "from PIL import ImageFile                            \n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# pre-process the data for Keras\n",
        "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
        "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
        "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6680/6680 [01:15<00:00, 88.11it/s]\n",
            "100%|██████████| 835/835 [00:08<00:00, 95.39it/s] \n",
            "100%|██████████| 836/836 [00:08<00:00, 102.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYWGgiGMybv1"
      },
      "source": [
        "## There are numerous other real-world factors that can affect our model:\n",
        "\n",
        "- Lighting condition: different lightings change how colors are displayed\n",
        "- Object orientation: our dogs can help many different poses\n",
        "- Picture frame: a close-up portrait frame is very different than full body-shot\n",
        "- Missing features: not all features of a dog is shown in a photo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpZtYuHCyXNv"
      },
      "source": [
        "# Create a model of your own"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtlBLDM6yxhi"
      },
      "source": [
        "The objective is to predict the breed of our dog image correctly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyhzj0GdyiNU"
      },
      "source": [
        "## Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70ERf8IryXeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c530f69-f74b-4eeb-de9e-ed3fbed54c05"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=16, kernel_size=2, padding='same',activation='relu',input_shape=(224,224,3)))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(133,activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 224, 224, 16)      208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 112, 112, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 112, 112, 32)      2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 56, 56, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 133)               8645      \n",
            "=================================================================\n",
            "Total params: 19,189\n",
            "Trainable params: 19,189\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFQ7StbPyu6_"
      },
      "source": [
        "## Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdRA9xv6yvNo"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvsF-0YkzJ9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18802385-2424-4805-da16-8b3f179f5e5c"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='weights.best.from_scratch.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(train_tensors, train_targets, \n",
        "          validation_data=(valid_tensors, valid_targets),\n",
        "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "334/334 [==============================] - ETA: 0s - loss: 4.7426 - accuracy: 0.0208\n",
            "Epoch 00001: val_loss improved from inf to 4.76818, saving model to weights.best.from_scratch.hdf5\n",
            "334/334 [==============================] - 217s 651ms/step - loss: 4.7426 - accuracy: 0.0208 - val_loss: 4.7682 - val_accuracy: 0.0192\n",
            "Epoch 2/5\n",
            "334/334 [==============================] - ETA: 0s - loss: 4.7185 - accuracy: 0.0253\n",
            "Epoch 00002: val_loss improved from 4.76818 to 4.74761, saving model to weights.best.from_scratch.hdf5\n",
            "334/334 [==============================] - 215s 645ms/step - loss: 4.7185 - accuracy: 0.0253 - val_loss: 4.7476 - val_accuracy: 0.0216\n",
            "Epoch 3/5\n",
            "334/334 [==============================] - ETA: 0s - loss: 4.6897 - accuracy: 0.0305\n",
            "Epoch 00003: val_loss improved from 4.74761 to 4.73671, saving model to weights.best.from_scratch.hdf5\n",
            "334/334 [==============================] - 214s 642ms/step - loss: 4.6897 - accuracy: 0.0305 - val_loss: 4.7367 - val_accuracy: 0.0228\n",
            "Epoch 4/5\n",
            "334/334 [==============================] - ETA: 0s - loss: 4.6663 - accuracy: 0.0302\n",
            "Epoch 00004: val_loss improved from 4.73671 to 4.71049, saving model to weights.best.from_scratch.hdf5\n",
            "334/334 [==============================] - 214s 641ms/step - loss: 4.6663 - accuracy: 0.0302 - val_loss: 4.7105 - val_accuracy: 0.0216\n",
            "Epoch 5/5\n",
            "334/334 [==============================] - ETA: 0s - loss: 4.6410 - accuracy: 0.0328\n",
            "Epoch 00005: val_loss improved from 4.71049 to 4.69433, saving model to weights.best.from_scratch.hdf5\n",
            "334/334 [==============================] - 216s 646ms/step - loss: 4.6410 - accuracy: 0.0328 - val_loss: 4.6943 - val_accuracy: 0.0240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faaaef1bdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sV_BhmI4_al"
      },
      "source": [
        "model.load_weights('weights.best.from_scratch.hdf5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpLUUFGo5CCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030d7ad0-2748-418b-c44d-1ad242da6b4e"
      },
      "source": [
        "# get index of predicted dog breed for each image in test set\n",
        "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
        "\n",
        "# report test accuracy\n",
        "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
        "print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 3.2297%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RN1bZtLzn30"
      },
      "source": [
        "# Create a model using transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9vuQbYIzpgx"
      },
      "source": [
        "## Obtain bottleneck features\n",
        "Keras offers the following pre-trained state-of-the-art architectures that you can use in minutes: VGG-19, ResNet-50, Inception, and Xception. In this project, we will use ResNet-50, but you can try out other architecture on your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQggqJoCzoJr"
      },
      "source": [
        "import requests\n",
        "url = 'https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz'\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('DogResnet50Data.npz', 'wb') as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "bottleneck_features = np.load('DogResnet50Data.npz')\n",
        "train_Resnet50 = bottleneck_features['train']\n",
        "valid_Resnet50 = bottleneck_features['valid']\n",
        "test_Resnet50 = bottleneck_features['test']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daacYMDV1mC7"
      },
      "source": [
        "In transfer learning, we take a pre-trained model (network + weights), and then removes the FC network, and construct our own in place of it. Doing so, it will remove the pre-trained weights at those layers. Now the original network before the FC network is frozen so that, when the training is start on the new data set, only the newly added FC network are trained. Here the input to the FC network is called the bottleneck features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWlvl0SKz23F"
      },
      "source": [
        "## Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LrhsDXLz2nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8b2997-be48-4a09-e379-3314691c2b7e"
      },
      "source": [
        "Resnet50_model = Sequential()\n",
        "Resnet50_model.add(GlobalAveragePooling2D(input_shape=train_Resnet50.shape[1:]))\n",
        "Resnet50_model.add(Dropout(0.3))\n",
        "Resnet50_model.add(Dense(1024,activation='relu'))\n",
        "Resnet50_model.add(Dropout(0.4))\n",
        "Resnet50_model.add(Dense(133, activation='softmax'))\n",
        "Resnet50_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 133)               136325    \n",
            "=================================================================\n",
            "Total params: 2,234,501\n",
            "Trainable params: 2,234,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCdhK7625Pq1"
      },
      "source": [
        "### TODO: Compile the model.\n",
        "Resnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_6PPl9b5TYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6113c8-2bcc-4703-c449-bf1ed169186c"
      },
      "source": [
        "### TODO: Train the model.\n",
        "checkpointer = ModelCheckpoint(filepath='weights.best.Resnet50.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "Resnet50_model.fit(train_Resnet50, train_targets, \n",
        "          validation_data=(valid_Resnet50, valid_targets),\n",
        "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "333/334 [============================>.] - ETA: 0s - loss: 2.3574 - accuracy: 0.4378\n",
            "Epoch 00001: val_loss improved from inf to 0.91071, saving model to weights.best.Resnet50.hdf5\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 2.3557 - accuracy: 0.4379 - val_loss: 0.9107 - val_accuracy: 0.7246\n",
            "Epoch 2/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 1.0687 - accuracy: 0.6994\n",
            "Epoch 00002: val_loss improved from 0.91071 to 0.84793, saving model to weights.best.Resnet50.hdf5\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 1.0687 - accuracy: 0.6994 - val_loss: 0.8479 - val_accuracy: 0.7401\n",
            "Epoch 3/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.7614\n",
            "Epoch 00003: val_loss improved from 0.84793 to 0.83980, saving model to weights.best.Resnet50.hdf5\n",
            "334/334 [==============================] - 9s 28ms/step - loss: 0.8270 - accuracy: 0.7614 - val_loss: 0.8398 - val_accuracy: 0.7880\n",
            "Epoch 4/20\n",
            "333/334 [============================>.] - ETA: 0s - loss: 0.7067 - accuracy: 0.7967\n",
            "Epoch 00004: val_loss improved from 0.83980 to 0.79796, saving model to weights.best.Resnet50.hdf5\n",
            "334/334 [==============================] - 10s 30ms/step - loss: 0.7061 - accuracy: 0.7970 - val_loss: 0.7980 - val_accuracy: 0.7880\n",
            "Epoch 5/20\n",
            "333/334 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.8294\n",
            "Epoch 00005: val_loss improved from 0.79796 to 0.75239, saving model to weights.best.Resnet50.hdf5\n",
            "334/334 [==============================] - 9s 28ms/step - loss: 0.6067 - accuracy: 0.8292 - val_loss: 0.7524 - val_accuracy: 0.8036\n",
            "Epoch 6/20\n",
            "333/334 [============================>.] - ETA: 0s - loss: 0.5944 - accuracy: 0.8405\n",
            "Epoch 00006: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 0.5943 - accuracy: 0.8406 - val_loss: 0.7833 - val_accuracy: 0.7940\n",
            "Epoch 7/20\n",
            "333/334 [============================>.] - ETA: 0s - loss: 0.5334 - accuracy: 0.8547\n",
            "Epoch 00007: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 0.5334 - accuracy: 0.8546 - val_loss: 0.8128 - val_accuracy: 0.8000\n",
            "Epoch 8/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.5024 - accuracy: 0.8680\n",
            "Epoch 00008: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 0.5024 - accuracy: 0.8680 - val_loss: 0.8665 - val_accuracy: 0.8084\n",
            "Epoch 9/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.8759\n",
            "Epoch 00009: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 0.4711 - accuracy: 0.8759 - val_loss: 0.8209 - val_accuracy: 0.8251\n",
            "Epoch 10/20\n",
            "332/334 [============================>.] - ETA: 0s - loss: 0.4432 - accuracy: 0.8825\n",
            "Epoch 00010: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 27ms/step - loss: 0.4430 - accuracy: 0.8823 - val_loss: 1.0213 - val_accuracy: 0.8024\n",
            "Epoch 11/20\n",
            "333/334 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8935\n",
            "Epoch 00011: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 0.4173 - accuracy: 0.8936 - val_loss: 0.9823 - val_accuracy: 0.8120\n",
            "Epoch 12/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.8978\n",
            "Epoch 00012: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 27ms/step - loss: 0.4089 - accuracy: 0.8978 - val_loss: 0.9750 - val_accuracy: 0.8251\n",
            "Epoch 13/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.4109 - accuracy: 0.8990\n",
            "Epoch 00013: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 0.4109 - accuracy: 0.8990 - val_loss: 1.0094 - val_accuracy: 0.8168\n",
            "Epoch 14/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.9075\n",
            "Epoch 00014: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 10s 29ms/step - loss: 0.3695 - accuracy: 0.9075 - val_loss: 1.0294 - val_accuracy: 0.8120\n",
            "Epoch 15/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.3799 - accuracy: 0.9103\n",
            "Epoch 00015: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 0.3799 - accuracy: 0.9103 - val_loss: 0.9736 - val_accuracy: 0.8192\n",
            "Epoch 16/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.9145\n",
            "Epoch 00016: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 26ms/step - loss: 0.3784 - accuracy: 0.9145 - val_loss: 1.0557 - val_accuracy: 0.8299\n",
            "Epoch 17/20\n",
            "333/334 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.9191\n",
            "Epoch 00017: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 27ms/step - loss: 0.3348 - accuracy: 0.9192 - val_loss: 1.2097 - val_accuracy: 0.8168\n",
            "Epoch 18/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.9177\n",
            "Epoch 00018: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 27ms/step - loss: 0.3369 - accuracy: 0.9177 - val_loss: 1.2154 - val_accuracy: 0.8012\n",
            "Epoch 19/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.9223\n",
            "Epoch 00019: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 27ms/step - loss: 0.3375 - accuracy: 0.9223 - val_loss: 1.2254 - val_accuracy: 0.8156\n",
            "Epoch 20/20\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.3489 - accuracy: 0.9213\n",
            "Epoch 00020: val_loss did not improve from 0.75239\n",
            "334/334 [==============================] - 9s 27ms/step - loss: 0.3489 - accuracy: 0.9213 - val_loss: 1.3335 - val_accuracy: 0.8144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faaae6e5b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8N6L-705ady"
      },
      "source": [
        "### TODO: Load the model weights with the best validation loss.\n",
        "Resnet50_model.load_weights('weights.best.Resnet50.hdf5')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzW47kOM1IZl"
      },
      "source": [
        "## Model performance\n",
        "Using transfer learning (ResNet-50), with twenty epochs and less than two minutes, we have achieved a test accuracy of 80.8612%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyN8--Ll5byV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77987513-3980-491f-93f1-7d5b76ef589c"
      },
      "source": [
        "# get index of predicted dog breed for each image in test set\n",
        "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\n",
        "\n",
        "# report test accuracy\n",
        "test_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\n",
        "print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 79.9043%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUPHznnt2dmu"
      },
      "source": [
        "# Future improvements\n",
        "- Augment data\n",
        "- Tune model\n",
        "- Try other models\n",
        "- Create a web/mobile app"
      ]
    }
  ]
}